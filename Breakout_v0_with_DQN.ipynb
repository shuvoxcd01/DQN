{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "le-TDxMoVSFV"
   },
   "source": [
    "DQN with experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T9YyhqQfVSFX",
    "outputId": "7eb99850-879d-4373-82f7-780c139e612d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import random\n",
    "import inspect\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from atari_py import ALEInterface, get_game_path, list_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzHrJlU9VSFd"
   },
   "outputs": [],
   "source": [
    "class EnvManager(ABC):\n",
    "    @abstractmethod\n",
    "    def get_legal_actions(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_random_action(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def initialize_input_sequence(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def execute_action(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_game_over(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation_shape(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlwt5-fPVSFi"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ALEManager(EnvManager):\n",
    "\n",
    "    def __init__(self, rom_name='Space_Invaders.bin', display_screen=False, frame_skip=3, color_averaging=True):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        self.ale = ALEInterface()\n",
    "        self.ale.setBool(b'display_screen', display_screen)\n",
    "        self.ale.setInt(b'frame_skip', frame_skip)\n",
    "        self.ale.setBool(b'color_averaging', color_averaging)\n",
    "        self._load_rom(rom_name)\n",
    "        self.actions = self.ale.getMinimalActionSet()\n",
    "        self.sequence = np.empty(shape=(84, 84, 4), dtype=np.uint8)\n",
    "\n",
    "    def _load_rom(self, rom_name):\n",
    "        if rom_name in list_games():\n",
    "            self.ale.loadROM(get_game_path(rom_name))\n",
    "            return\n",
    "\n",
    "        rom_path = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'ROMs', rom_name)\n",
    "        if not os.path.exists(rom_path):\n",
    "            self.logger.error(\"Invalid ROM path\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        self.ale.loadROM(bytes(rom_path, encoding='utf-8'))\n",
    "\n",
    "    def _map_action(self, action):\n",
    "        return self.actions[action]\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        return np.arange(len(self.actions), dtype=np.int32)\n",
    "\n",
    "    def get_random_action(self):\n",
    "        return random.choice(self.get_legal_actions())\n",
    "\n",
    "    def initialize_input_sequence(self):\n",
    "        self.ale.reset_game()\n",
    "        screen = np.empty((210, 160), dtype=np.uint8)\n",
    "        for i in range(4):\n",
    "            self.ale.act(self._map_action(self.get_random_action()))\n",
    "            self.ale.getScreenGrayscale(screen)\n",
    "            preprocessed_screen = self.preprocess_screen(screen)\n",
    "            self.sequence[:, :, i] = preprocessed_screen\n",
    "        return self.sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_screen(screen):\n",
    "        resized_screen = cv2.resize(screen, dsize=(84, 110), interpolation=cv2.INTER_AREA)\n",
    "        cropped_screen = resized_screen[17:110 - 9, :]\n",
    "        return cropped_screen\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Executes the action given as parameter and returns a\n",
    "        reward and a sequence of length 4 containing preprocessed screens.\"\"\"\n",
    "        screen = np.empty((210, 160), dtype=np.uint8)\n",
    "        reward = self.ale.act(self._map_action(action))\n",
    "        self.ale.getScreenGrayscale(screen)\n",
    "        preprocessed_screen = self.preprocess_screen(screen)\n",
    "        self.sequence[:, :, :3] = self.sequence[:, :, 1:]\n",
    "        self.sequence[:, :, -1] = preprocessed_screen\n",
    "\n",
    "        return reward, self.sequence\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return self.ale.game_over()\n",
    "\n",
    "    def get_observation_shape(self):\n",
    "        return (84, 84, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUnH5C38VSFn"
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, input_shape, output_units, save_model_dir='models', save_model_name='model.h5',\n",
    "                 load_model_dir=None, load_model_name=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_units = output_units\n",
    "        self.save_model_dir = save_model_dir\n",
    "        self.save_model_name = save_model_name\n",
    "        self.load_model_dir = load_model_dir\n",
    "        self.load_model_name = load_model_name\n",
    "        self.model = self._load_model()\n",
    "\n",
    "        if not os.path.exists(self.save_model_dir):\n",
    "            os.makedirs(self.save_model_dir)\n",
    "\n",
    "    def _load_model(self):\n",
    "        if self.load_model_dir is None or self.load_model_name is None:\n",
    "            print(\"Creating new neural-network\")\n",
    "            return self.get_q_network()\n",
    "\n",
    "        model_name = os.path.join(self.load_model_dir, self.load_model_name)\n",
    "\n",
    "        if os.path.exists(model_name):\n",
    "            print(\"Loading existing model, \" + str(model_name))\n",
    "            return load_model(model_name)\n",
    "\n",
    "        raise Exception(\"Model could not be loaded.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_q_network(self):\n",
    "        pass\n",
    "\n",
    "    def get_prediction(self, preprocessed_input):\n",
    "        return self.model.predict(np.expand_dims(preprocessed_input, 0))[0]\n",
    "\n",
    "    def get_predicted_action(self, preprocessed_input):\n",
    "        return np.argmax(self.get_prediction(preprocessed_input))\n",
    "\n",
    "    def prepare_minibatch(self, transitions_minibatch, gamma):\n",
    "        expected_output_minibatch = []\n",
    "        input_minibatch = []\n",
    "\n",
    "        for current_input, action, reward, next_input, is_terminal_state in transitions_minibatch:\n",
    "            q_value = reward\n",
    "            if not is_terminal_state:\n",
    "                q_value += gamma * np.amax(self.get_prediction(next_input))\n",
    "            prediction = self.get_prediction(current_input)\n",
    "            prediction[action] = q_value\n",
    "            expected_output_minibatch.append(prediction)\n",
    "            input_minibatch.append(current_input)\n",
    "\n",
    "        expected_output_minibatch = np.array(expected_output_minibatch)\n",
    "        input_minibatch = np.array(input_minibatch)\n",
    "\n",
    "        return input_minibatch, expected_output_minibatch\n",
    "\n",
    "    def perform_gradient_descent_step(self, _input, _output):\n",
    "        self.model.fit(x=_input, y=_output, epochs=1, verbose=0)\n",
    "\n",
    "    def save_model(self, step=''):\n",
    "        model_name = os.path.join(self.save_model_dir, (str(step) + '--' + self.save_model_name))\n",
    "        self.model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d58Wa_zsVSFr"
   },
   "outputs": [],
   "source": [
    "class DQNSpaceInvaders(DQN):\n",
    "\n",
    "    def __init__(self, input_shape, output_units, save_model_dir=\"models/space_invaders\", save_model_name='model.h5',\n",
    "                 load_model_dir=None, load_model_name=None):\n",
    "        super().__init__(input_shape, output_units, save_model_dir=save_model_dir, save_model_name=save_model_name,\n",
    "                         load_model_dir=load_model_dir, load_model_name=load_model_name)\n",
    "\n",
    "    def get_q_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Conv2D(filters=16, kernel_size=(8, 8), strides=(4, 4), input_shape=self.input_shape, activation='relu'))\n",
    "        model.add(Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=256, activation='relu'))\n",
    "        model.add(Dense(units=self.output_units))\n",
    "\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop())\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfeoPtEnVSFv"
   },
   "outputs": [],
   "source": [
    "class DQNBreakout(DQNSpaceInvaders):\n",
    "    def __init__(self, input_shape, output_units, save_model_dir, save_model_name, load_model_dir, load_model_name):\n",
    "        super().__init__(input_shape, output_units, save_model_dir, save_model_name, load_model_dir, load_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_yok2OCVSFz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DeepQLearningAgent(object):\n",
    "    def __init__(self, env_manager=ALEManager, q_network=DQNSpaceInvaders, num_total_episode=10000,\n",
    "                 episode_starts_from=0, epsilon_decay_rate=9.000000000000001e-07, save_model_step=100, epsilon=1.,\n",
    "                 logdir=None):\n",
    "        self.minibatch_size = 32\n",
    "        self.experience_replay_memory = deque([], maxlen=1000000)\n",
    "        self.env_manager = env_manager() if inspect.isclass(env_manager) else env_manager\n",
    "        self.possible_actions = self.env_manager.get_legal_actions()\n",
    "        self.input_shape = self.env_manager.get_observation_shape()\n",
    "        self.output_units = len(self.possible_actions)\n",
    "        self.DQN = q_network(input_shape=self.input_shape, output_units=self.output_units) if inspect.isclass(\n",
    "            q_network) else q_network\n",
    "        self.epsilon = float(epsilon)\n",
    "        self.gamma = 0.9\n",
    "        self.num_total_episode = num_total_episode\n",
    "        self.n_episode = episode_starts_from\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") if logdir is None else logdir\n",
    "        self.file_writer = tf.summary.create_file_writer(logdir=logdir)\n",
    "        self.save_model_step = save_model_step\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon < 0.1:\n",
    "            self.epsilon = 0.1\n",
    "            return\n",
    "        elif self.epsilon == 0.1:\n",
    "            return\n",
    "        else:\n",
    "            self.epsilon -= self.epsilon_decay_rate\n",
    "\n",
    "    def e_greedy_select_action(self, preprocessed_input):\n",
    "        if random.random() <= self.epsilon:\n",
    "            action = self.env_manager.get_random_action()\n",
    "        else:\n",
    "            action = self.DQN.get_predicted_action(preprocessed_input)\n",
    "\n",
    "        self.update_epsilon()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn_with_experience_replay(self):\n",
    "        \"\"\"vanilla deep_q_learning_with_experience_replay\"\"\"\n",
    "        while self.n_episode < self.num_total_episode:\n",
    "            preprocessed_input = self.env_manager.initialize_input_sequence()\n",
    "            cumulative_reward = 0\n",
    "            episode_q_value_list = []\n",
    "            while not self.env_manager.is_game_over():\n",
    "                action = self.e_greedy_select_action(preprocessed_input)\n",
    "                reward, next_preprocessed_input = self.env_manager.execute_action(action)\n",
    "\n",
    "                cumulative_reward += reward\n",
    "                q_value_for_selected_action = self.DQN.get_prediction(preprocessed_input)[action]\n",
    "                episode_q_value_list.append(q_value_for_selected_action)\n",
    "\n",
    "                self.experience_replay_memory.append(\n",
    "                    (preprocessed_input, action, reward, next_preprocessed_input, self.env_manager.is_game_over()))\n",
    "\n",
    "                preprocessed_input = next_preprocessed_input\n",
    "\n",
    "                if len(self.experience_replay_memory) > self.minibatch_size:\n",
    "                    sample_minibatch = random.sample(self.experience_replay_memory, k=self.minibatch_size)\n",
    "                    _input, _output = self.DQN.prepare_minibatch(sample_minibatch, self.gamma)\n",
    "                    self.DQN.perform_gradient_descent_step(_input, _output)\n",
    "\n",
    "            avg_q_value_per_action = sum(episode_q_value_list) / float(len(episode_q_value_list))\n",
    "\n",
    "            with self.file_writer.as_default():\n",
    "                tf.summary.scalar('Return per episode', cumulative_reward, step=self.n_episode)\n",
    "                tf.summary.scalar('Average q_value', avg_q_value_per_action, step=self.n_episode)\n",
    "                tf.summary.scalar('epsilon', self.epsilon, step=self.n_episode)\n",
    "                tf.summary.flush()\n",
    "\n",
    "            if ((self.n_episode + 1) % self.save_model_step) == 0:\n",
    "                self.DQN.save_model('-episode:' + str(self.n_episode + 1) + '-epsilon:' + str(self.epsilon))\n",
    "\n",
    "            self.n_episode += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAeYgIFpVbLF"
   },
   "outputs": [],
   "source": [
    "model_dir = \"models/breakout/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(model_dir + \" does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1Ls7ObyVeR2"
   },
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/breakout/\"\n",
    "if not os.path.exists(logdir):\n",
    "    print(logdir + \" does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RIODkbzsVSF8",
    "outputId": "823d5448-eb1e-4abd-e3a1-4d5e17de0b01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model, models/breakout/-episode_5100-epsilon_0.13606480000951018--breakout_dqn.h5\n"
     ]
    }
   ],
   "source": [
    "env_manager = ALEManager(rom_name='breakout', frame_skip=4)\n",
    "\n",
    "save_model_dir = model_dir\n",
    "save_model_name=\"breakout_dqn.h5\"\n",
    "load_model_dir = model_dir\n",
    "load_model_name = '-episode_5100-epsilon_0.13606480000951018--breakout_dqn.h5'\n",
    "episode_start_from = 5100 + 1\n",
    "epsilon = 0.13606480000951018\n",
    "input_shape=env_manager.get_observation_shape()\n",
    "output_units = len(env_manager.get_legal_actions())\n",
    "q_network = DQNBreakout(input_shape=input_shape, output_units=output_units, save_model_dir=save_model_dir, save_model_name=save_model_name, load_model_dir=load_model_dir, load_model_name=load_model_name)\n",
    "\n",
    "num_total_episode = 10000\n",
    "epsilon_decay_rate=9.000000000000001e-07\n",
    "save_model_step=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LszFiYAVSF_"
   },
   "outputs": [],
   "source": [
    "breakout_agent = DeepQLearningAgent(env_manager=env_manager, q_network=q_network, num_total_episode=num_total_episode, epsilon_decay_rate=epsilon_decay_rate, save_model_step=save_model_step, epsilon=epsilon, logdir=logdir, episode_starts_from=episode_start_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wK0OWzVDVSGD"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f8764ce2103b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbreakout_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_with_experience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-91b8e1b734d7>\u001b[0m in \u001b[0;36mlearn_with_experience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay_memory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0msample_minibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_minibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_gradient_descent_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-97dc74f8aa4a>\u001b[0m in \u001b[0;36mprepare_minibatch\u001b[0;34m(self, transitions_minibatch, gamma)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_terminal_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mq_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mexpected_output_minibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-97dc74f8aa4a>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(self, preprocessed_input)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_predicted_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl-gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/rl-gpu/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/rl-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "breakout_agent.learn_with_experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Breakout-v0_with_DQN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
