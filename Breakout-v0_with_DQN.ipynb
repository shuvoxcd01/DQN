{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN with experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import random\n",
    "import inspect\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from atari_py import ALEInterface, get_game_path, list_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager(ABC):\n",
    "    @abstractmethod\n",
    "    def get_legal_actions(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_random_action(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def initialize_input_sequence(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def execute_action(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_game_over(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation_shape(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALEManager(EnvManager):\n",
    "\n",
    "    def __init__(self, rom_name='Space_Invaders.bin', display_screen=False, frame_skip=3, color_averaging=True):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        self.ale = ALEInterface()\n",
    "        self.ale.setBool(b'display_screen', display_screen)\n",
    "        self.ale.setInt(b'frame_skip', frame_skip)\n",
    "        self.ale.setBool(b'color_averaging', color_averaging)\n",
    "        self._load_rom(rom_name)\n",
    "\n",
    "        self.sequence = np.empty(shape=(84, 84, 4), dtype=np.uint8)\n",
    "\n",
    "    def _load_rom(self, rom_name):\n",
    "        if rom_name in list_games():\n",
    "            self.ale.loadROM(get_game_path(rom_name))\n",
    "            return\n",
    "\n",
    "        rom_path = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'ROMs', rom_name)\n",
    "        if not os.path.exists(rom_path):\n",
    "            self.logger.error(\"Invalid ROM path\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        self.ale.loadROM(bytes(rom_path, encoding='utf-8'))\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        return self.ale.getLegalActionSet()\n",
    "\n",
    "    def get_random_action(self):\n",
    "        return random.choice(self.get_legal_actions())\n",
    "\n",
    "    def initialize_input_sequence(self):\n",
    "        self.ale.reset_game()\n",
    "        screen = np.empty((210, 160), dtype=np.uint8)\n",
    "        for i in range(4):\n",
    "            self.ale.act(self.get_random_action())\n",
    "            self.ale.getScreenGrayscale(screen)\n",
    "            preprocessed_screen = self.preprocess_screen(screen)\n",
    "            self.sequence[:, :, i] = preprocessed_screen\n",
    "        return self.sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_screen(screen):\n",
    "        resized_screen = cv2.resize(screen, dsize=(84, 110), interpolation=cv2.INTER_AREA)\n",
    "        cropped_screen = resized_screen[17:110 - 9, :]\n",
    "        return cropped_screen\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        \"\"\"Executes the action given as parameter and returns a\n",
    "        reward and a sequence of length 4 containing preprocessed screens.\"\"\"\n",
    "        screen = np.empty((210, 160), dtype=np.uint8)\n",
    "        reward = self.ale.act(action)\n",
    "        self.ale.getScreenGrayscale(screen)\n",
    "        preprocessed_screen = self.preprocess_screen(screen)\n",
    "        self.sequence[:, :, :3] = self.sequence[:, :, 1:]\n",
    "        self.sequence[:, :, -1] = preprocessed_screen\n",
    "\n",
    "        return reward, self.sequence\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return self.ale.game_over()\n",
    "\n",
    "    def get_observation_shape(self):\n",
    "        return (84, 84, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, input_shape, output_units, save_model_dir='models', save_model_name='model.h5',\n",
    "                 load_model_dir=None, load_model_name=None):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_units = output_units\n",
    "        self.save_model_dir = save_model_dir\n",
    "        self.save_model_name = save_model_name\n",
    "        self.load_model_dir = load_model_dir\n",
    "        self.load_model_name = load_model_name\n",
    "        self.model = self._load_model()\n",
    "\n",
    "        if not os.path.exists(self.save_model_dir):\n",
    "            os.makedirs(self.save_model_dir)\n",
    "\n",
    "    def _load_model(self):\n",
    "        if self.load_model_dir is None and self.load_model_name is None:\n",
    "            return self.get_q_network()\n",
    "\n",
    "        model_name = os.path.join(self.load_model_dir, self.load_model_name)\n",
    "\n",
    "        if os.path.exists(model_name):\n",
    "            return load_model(model_name)\n",
    "\n",
    "        raise Exception(\"Model could not be loaded.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_q_network(self):\n",
    "        pass\n",
    "\n",
    "    def get_prediction(self, preprocessed_input):\n",
    "        return self.model.predict(np.expand_dims(preprocessed_input, 0))[0]\n",
    "\n",
    "    def get_predicted_action(self, preprocessed_input):\n",
    "        return np.argmax(self.get_prediction(preprocessed_input))\n",
    "\n",
    "    def prepare_minibatch(self, transitions_minibatch, gamma):\n",
    "        expected_output_minibatch = []\n",
    "        input_minibatch = []\n",
    "\n",
    "        for current_input, action, reward, next_input, is_terminal_state in transitions_minibatch:\n",
    "            q_value = reward\n",
    "            if not is_terminal_state:\n",
    "                q_value += gamma * np.amax(self.get_prediction(next_input))\n",
    "            prediction = self.get_prediction(current_input)\n",
    "            prediction[action] = q_value\n",
    "            expected_output_minibatch.append(prediction)\n",
    "            input_minibatch.append(current_input)\n",
    "\n",
    "        expected_output_minibatch = np.array(expected_output_minibatch)\n",
    "        input_minibatch = np.array(input_minibatch)\n",
    "\n",
    "        return input_minibatch, expected_output_minibatch\n",
    "\n",
    "    def perform_gradient_descent_step(self, _input, _output):\n",
    "        self.model.fit(x=_input, y=_output, epochs=1)\n",
    "\n",
    "    def save_model(self, step=''):\n",
    "        model_name = os.path.join(self.save_model_dir, (str(step) + '--' + self.save_model_name))\n",
    "        self.model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSpaceInvaders(DQN):\n",
    "\n",
    "    def __init__(self, input_shape, output_units, save_model_dir=\"models/space_invaders\", save_model_name='model.h5',\n",
    "                 load_model_dir=None, load_model_name=None):\n",
    "        super().__init__(input_shape, output_units, save_model_dir=save_model_dir, save_model_name=save_model_name,\n",
    "                         load_model_dir=load_model_dir, load_model_name=load_model_name)\n",
    "\n",
    "    def get_q_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Conv2D(filters=16, kernel_size=(8, 8), strides=(4, 4), input_shape=self.input_shape, activation='relu'))\n",
    "        model.add(Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=256, activation='relu'))\n",
    "        model.add(Dense(units=self.output_units))\n",
    "\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop())\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNBreakout(DQNSpaceInvaders):\n",
    "    def __init__(self, input_shape, output_units, save_model_dir, save_model_name, load_model_dir, load_model_name):\n",
    "        super().__init__(input_shape, output_units, save_model_dir, save_model_name, load_model_dir, load_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepQLearningAgent(object):\n",
    "    def __init__(self, env_manager=ALEManager, q_network=DQNSpaceInvaders, num_total_episode=10000,\n",
    "                 episode_starts_from=0, epsilon_decay_rate=9.000000000000001e-07, save_model_step=100, epsilon=1.,\n",
    "                 logdir=None):\n",
    "        self.minibatch_size = 32\n",
    "        self.experience_replay_memory = deque([], maxlen=1000000)\n",
    "        self.env_manager = env_manager() if inspect.isclass(env_manager) else env_manager\n",
    "        self.possible_actions = self.env_manager.get_legal_actions()\n",
    "        self.input_shape = self.env_manager.get_observation_shape()\n",
    "        self.output_units = len(self.possible_actions)\n",
    "        self.DQN = q_network(input_shape=self.input_shape, output_units=self.output_units) if inspect.isclass(\n",
    "            q_network) else q_network\n",
    "        self.epsilon = float(epsilon)\n",
    "        self.gamma = 0.9\n",
    "        self.num_total_episode = num_total_episode\n",
    "        self.n_episode = episode_starts_from\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") if logdir is None else logdir\n",
    "        self.file_writer = tf.summary.create_file_writer(logdir=logdir)\n",
    "        self.save_model_step = save_model_step\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon < 0.1:\n",
    "            self.epsilon = 0.1\n",
    "            return\n",
    "        elif self.epsilon == 0.1:\n",
    "            return\n",
    "        else:\n",
    "            self.epsilon -= self.epsilon_decay_rate\n",
    "\n",
    "    def e_greedy_select_action(self, preprocessed_input):\n",
    "        if random.random() <= self.epsilon:\n",
    "            action = self.env_manager.get_random_action()\n",
    "        else:\n",
    "            action = self.DQN.get_predicted_action(preprocessed_input)\n",
    "\n",
    "        self.update_epsilon()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn_with_experience_replay(self):\n",
    "        \"\"\"vanilla deep_q_learning_with_experience_replay\"\"\"\n",
    "        while self.n_episode < self.num_total_episode:\n",
    "            preprocessed_input = self.env_manager.initialize_input_sequence()\n",
    "            cumulative_reward = 0\n",
    "            episode_q_value_list = []\n",
    "            while not self.env_manager.is_game_over():\n",
    "                action = self.e_greedy_select_action(preprocessed_input)\n",
    "                reward, next_preprocessed_input = self.env_manager.execute_action(action)\n",
    "\n",
    "                cumulative_reward += reward\n",
    "                q_value_for_selected_action = self.DQN.get_prediction(preprocessed_input)[action]\n",
    "                episode_q_value_list.append(q_value_for_selected_action)\n",
    "\n",
    "                self.experience_replay_memory.append(\n",
    "                    (preprocessed_input, action, reward, next_preprocessed_input, self.env_manager.is_game_over()))\n",
    "\n",
    "                preprocessed_input = next_preprocessed_input\n",
    "\n",
    "                if len(self.experience_replay_memory) > self.minibatch_size:\n",
    "                    sample_minibatch = random.sample(self.experience_replay_memory, k=self.minibatch_size)\n",
    "                    _input, _output = self.DQN.prepare_minibatch(sample_minibatch, self.gamma)\n",
    "                    self.DQN.perform_gradient_descent_step(_input, _output)\n",
    "\n",
    "            avg_q_value_per_action = sum(episode_q_value_list) / float(len(episode_q_value_list))\n",
    "\n",
    "            with self.file_writer.as_default():\n",
    "                tf.summary.scalar('Return per episode', cumulative_reward, step=self.n_episode)\n",
    "                tf.summary.scalar('Average q_value', avg_q_value_per_action, step=self.n_episode)\n",
    "                tf.summary.scalar('epsilon', self.epsilon, step=self.n_episode)\n",
    "                tf.summary.flush()\n",
    "\n",
    "            if ((self.n_episode + 1) % self.save_model_step) == 0:\n",
    "                self.DQN.save_model('-episode:' + str(self.n_episode + 1) + '-epsilon:' + str(self.epsilon))\n",
    "\n",
    "            self.n_episode += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_manager = ALEManager(rom_name='breakout', frame_skip=4)\n",
    "\n",
    "save_model_dir = \"models/breakout\"\n",
    "save_model_name=\"breakout_dqn.h5\"\n",
    "load_model_dir = None\n",
    "load_model_name = None\n",
    "input_shape=env_manager.get_observation_shape()\n",
    "output_units = len(env_manager.get_legal_actions())\n",
    "q_network = DQNBreakout(input_shape=input_shape, output_units=output_units, save_model_dir=save_model_dir, save_model_name=save_model_name, load_model_dir=load_model_dir, load_model_name=load_model_name)\n",
    "\n",
    "num_total_episode = 10000\n",
    "episode_start_from = 0\n",
    "epsilon_decay_rate=9.000000000000001e-07\n",
    "save_model_step=100\n",
    "epsilon=1.\n",
    "logdir = \"logs/scalars/breakout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakout_agent = DeepQLearningAgent(env_manager=env_manager, q_network=q_network, num_total_episode=num_total_episode, epsilon_decay_rate=epsilon_decay_rate, save_model_step=save_model_step, epsilon=epsilon, logdir=logdir, episode_starts_from=episode_start_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakout_agent.learn_with_experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
