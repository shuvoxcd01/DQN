{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import logging\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack\n",
    "\n",
    "from abc import ABC, abstractmethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionTable(object):\n",
    "    def __init__(self, maxlen=100000):\n",
    "        self.transitions = deque(maxlen=maxlen)\n",
    "\n",
    "    def sample(self, size=1):\n",
    "        assert len(self.transitions) >= size\n",
    "        samples = random.sample(self.transitions, size)\n",
    "\n",
    "        s = np.empty(shape=(size, 84, 84, 4), dtype=np.float32)\n",
    "        s2 = np.empty(shape=(size, 84, 84, 4), dtype=np.float32)\n",
    "        a = np.empty(shape=size, dtype=np.int32)\n",
    "        r = np.empty(shape=size, dtype=np.float32)\n",
    "        term = np.empty(shape=size, dtype=np.float32)\n",
    "\n",
    "        for i in range(size):\n",
    "            s[i] = samples[i][0]\n",
    "            a[i] = samples[i][1]\n",
    "            r[i] = samples[i][2]\n",
    "            s2[i] = samples[i][3]\n",
    "            term[i] = samples[i][4]\n",
    "\n",
    "        s = tf.Variable(s, dtype=tf.float32)\n",
    "        a = tf.Variable(a, dtype=tf.int32)\n",
    "        r = tf.Variable(r, dtype=tf.float32)\n",
    "        s2 = tf.Variable(s2, dtype=tf.float32)\n",
    "        term = tf.Variable(term, dtype=tf.float32)\n",
    "\n",
    "        return s, a, r, s2, term\n",
    "\n",
    "    def add(self, s, a, r, s2, is_term):\n",
    "        term = 1 if is_term else 0\n",
    "        self.transitions.append((s, a, r, s2, term))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager(ABC):\n",
    "    @abstractmethod\n",
    "    def get_legal_actions(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_random_action(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def initialize_input_sequence(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def execute_action(self, action):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_game_over(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ALEManagerArgs(object):\n",
    "    def __init__(self):\n",
    "        self.ROM_NAME = 'PongNoFrameskip-v4'\n",
    "        self.SHOW_SCREEN = False\n",
    "        self.SCREEN_SIZE = 84\n",
    "        self.FRAME_SKIP = 4  # ACTION_REPEAT\n",
    "        self.COLOR_AVERAGING = True\n",
    "        self.NO_OP_MAX = 30\n",
    "        self.AGENT_HISTORY_LENGTH = 4\n",
    "        self.TERMINAL_ON_LIFE_LOSS = True\n",
    "        self.GRAYSCALE_OBS = True\n",
    "        self.SCALE_OBS = False\n",
    "\n",
    "\n",
    "class ALEManager(EnvManager):\n",
    "    def __init__(self, args):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.ROM_NAME = args.ROM_NAME\n",
    "        self.SHOW_SCREEN = args.SHOW_SCREEN\n",
    "        self.SCREEN_SIZE = args.SCREEN_SIZE\n",
    "        self.FRAME_SKIP = args.FRAME_SKIP\n",
    "        self.COLOR_AVERAGING = args.COLOR_AVERAGING\n",
    "        self.NO_OP_MAX = args.NO_OP_MAX\n",
    "        self.AGENT_HISTORY_LENGTH = args.AGENT_HISTORY_LENGTH\n",
    "        self.TERMINAL_ON_LIFE_LOSS = args.TERMINAL_ON_LIFE_LOSS\n",
    "        self.GRAYSCALE_OBS = args.GRAYSCALE_OBS\n",
    "        self.SCALE_OBS = args.SCALE_OBS\n",
    "\n",
    "        env = gym.make(self.ROM_NAME)\n",
    "        env = AtariPreprocessing(env, noop_max=self.NO_OP_MAX, frame_skip=self.FRAME_SKIP, screen_size=self.SCREEN_SIZE,\n",
    "                                 terminal_on_life_loss=self.TERMINAL_ON_LIFE_LOSS, grayscale_obs=self.GRAYSCALE_OBS,\n",
    "                                 scale_obs=self.SCALE_OBS)\n",
    "\n",
    "        self.env = FrameStack(env, num_stack=self.AGENT_HISTORY_LENGTH)\n",
    "\n",
    "        self.cur_obs = None\n",
    "        self.cur_reward = 0\n",
    "        self.done = False\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        return np.arange(self.env.action_space.n, dtype=np.int32)\n",
    "\n",
    "    def get_random_action(self):\n",
    "        return self.env.action_space.sample()\n",
    "\n",
    "    def initialize_input_sequence(self):\n",
    "        self.cur_obs = self.env.reset()\n",
    "        self.done = False\n",
    "        return np.moveaxis(self.cur_obs, 0, -1)\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        if self.SHOW_SCREEN:\n",
    "            self.env.render()\n",
    "        self.cur_obs, self.cur_reward, self.done, info = self.env.step(action)\n",
    "        return self.cur_reward, np.moveaxis(self.cur_obs, 0, -1)\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearningAgent(object):\n",
    "    def __init__(self, args):\n",
    "        self.env = args.env\n",
    "        assert self.env is not None, \"Environment not given.\"\n",
    "        self.actions = self.env.get_legal_actions()\n",
    "        self.n_actions = len(self.actions)\n",
    "\n",
    "        self.write_weight_histogram = args.write_weight_histogram\n",
    "        self.network = args.network if args.network is not None else self.create_network()\n",
    "        self.target_network = tf.keras.models.clone_model(self.network)\n",
    "\n",
    "        self.total_steps = args.total_steps\n",
    "        self.update_freq = args.update_freq\n",
    "        self.learn_start = args.learn_start\n",
    "        self.epsilon_start = args.epsilon_start\n",
    "        self.epsilon_end = args.epsilon_end\n",
    "        self.epsilon_endt = args.epsilon_endt\n",
    "        self.discount = args.discount\n",
    "        self.num_steps = 0\n",
    "        self.epsilon = self.epsilon_start\n",
    "        self.rescale_r = args.rescale_r\n",
    "        self.r_max = args.r_max\n",
    "\n",
    "        self.minibatch_size = args.minibatch_size\n",
    "        self.target_q = args.target_q\n",
    "\n",
    "        assert self.r_max is not None if self.rescale_r is not None else True, \"R_MAX not defined\"\n",
    "\n",
    "        # save model\n",
    "        self.save_model_steps = args.save_model_steps\n",
    "        self.save_model_path = args.save_model_path if args.save_model_path is not None else \"models/\"\n",
    "        if not os.path.exists(self.save_model_path):\n",
    "            os.makedirs(self.save_model_path)\n",
    "\n",
    "        # learning rate annealing\n",
    "        self.lr_start = 0.01 if args.lr is None else args.lr\n",
    "        self.lr = self.lr_start\n",
    "        self.lr_end = self.lr if args.lr_end is None else args.lr_end\n",
    "        self.lr_endt = 1000000 if args.lr_endt is None else args.lr_endt\n",
    "\n",
    "        self.deltas = []\n",
    "        self.tmp = []\n",
    "        self.g = []\n",
    "        self.g2 = []\n",
    "        for i in range(len(self.network.weights)):\n",
    "            self.deltas.append(tf.zeros_like(self.network.weights[i]))\n",
    "            self.tmp.append(tf.zeros_like(self.network.weights[i]))\n",
    "            self.g.append(tf.zeros_like(self.network.weights[i]))\n",
    "            self.g2.append(tf.zeros_like(self.network.weights[i]))\n",
    "\n",
    "        self.experience_replay_memory = TransitionTable()\n",
    "\n",
    "        cur_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        logdir_scalar = \"logs/scalars/\" + cur_datetime\n",
    "        logdir_histogram = \"logs/histograms/\" + cur_datetime\n",
    "        self.scalar_file_writer = tf.summary.create_file_writer(logdir=logdir_scalar)\n",
    "        self.histogram_file_writer = tf.summary.create_file_writer(logdir=logdir_histogram)\n",
    "\n",
    "    def create_network(self):\n",
    "        network = keras.models.Sequential()\n",
    "        network.add(\n",
    "            keras.layers.Conv2D(filters=32, kernel_size=(8, 8), strides=4, activation='relu', input_shape=(84, 84, 4)))\n",
    "        network.add(keras.layers.Conv2D(filters=64, kernel_size=(4, 4), strides=2, activation='relu'))\n",
    "        network.add(keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu'))\n",
    "        network.add(keras.layers.Flatten())\n",
    "        network.add(keras.layers.Dense(units=512, activation='relu'))\n",
    "        network.add(keras.layers.Dense(units=self.n_actions))\n",
    "\n",
    "        self.write_weight_histogram = True\n",
    "\n",
    "        return network\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = self.epsilon_end + max(0, (self.epsilon_start - self.epsilon_end) * (\n",
    "                self.epsilon_endt - max(0, self.num_steps - self.learn_start)) / self.epsilon_endt)\n",
    "\n",
    "    def e_greedy_select_action(self, preprocessed_input):\n",
    "        if random.random() <= self.epsilon:\n",
    "            action = self.env.get_random_action()\n",
    "        else:\n",
    "            action = np.argmax(self.network.predict(np.expand_dims(preprocessed_input, 0))[0])\n",
    "\n",
    "        self.update_epsilon()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_q_update(self, s, a, r, s2, term):\n",
    "        term = tf.add(tf.multiply(tf.cast(tf.identity(term), tf.float32), -1), 1)\n",
    "        q2_max = tf.reduce_max(tf.cast(tf.identity(self.target_network(s2)), tf.float32), axis=1)\n",
    "        q2 = tf.multiply(tf.multiply(tf.identity(q2_max), self.discount), term)\n",
    "        delta = tf.cast(tf.identity(r), tf.float32)\n",
    "        if self.rescale_r:\n",
    "            delta = tf.divide(delta, self.r_max)\n",
    "        delta = tf.add(delta, q2)\n",
    "        q_all = tf.cast(self.network(s), tf.float32)\n",
    "        q = np.empty(shape=q_all.shape[0], dtype=np.float32)\n",
    "\n",
    "        for i in range(q_all.shape[0]):\n",
    "            q[i] = (q_all[i][a[i]])\n",
    "\n",
    "        q = tf.Variable(q)\n",
    "        delta = tf.add(delta, tf.multiply(q, -1))\n",
    "\n",
    "        # clip delta not applied\n",
    "\n",
    "        targets = np.zeros(shape=(self.minibatch_size, self.n_actions), dtype=np.float32)\n",
    "\n",
    "        for i in range(min(self.minibatch_size, a.shape[0])):\n",
    "            targets[i][a[i]] = delta[i]\n",
    "\n",
    "        targets = tf.Variable(targets)\n",
    "\n",
    "        return targets, delta, q2_max\n",
    "\n",
    "    def q_learn_minibatch(self, s, a, r, s2, term):\n",
    "        targets, delta, q2_max = self.get_q_update(s, a, r, s2, term)\n",
    "        with tf.GradientTape() as t:\n",
    "            y_hat = self.network(s)\n",
    "\n",
    "        dw = t.gradient(y_hat, self.network.weights, output_gradients=targets)\n",
    "\n",
    "        # Ignoring weight cost\n",
    "\n",
    "        # compute linearly annealed learning rate\n",
    "        t = max(0, self.num_steps - self.learn_start)\n",
    "        self.lr = (self.lr_start - self.lr_end) * (self.lr_endt - t) / self.lr_endt + self.lr_end\n",
    "        self.lr = max(self.lr, self.lr_end)\n",
    "\n",
    "        assert len(dw) == len(self.network.weights), \"len(dw) and len(network.weights) does not match\"\n",
    "\n",
    "        tmp_weights = []\n",
    "\n",
    "        for i in range(len(self.network.weights)):\n",
    "            self.g[i] = tf.multiply(self.g[i], 0.95) + tf.multiply(0.05, dw[i])\n",
    "            self.tmp[i] = tf.multiply(dw[i], dw[i])\n",
    "            self.g2[i] = tf.multiply(self.g2[i], 0.95) + tf.multiply(0.05, self.tmp[i])\n",
    "            self.tmp[i] = tf.multiply(self.g[i], self.g[i])\n",
    "            self.tmp[i] = tf.multiply(self.tmp[i], -1)\n",
    "            self.tmp[i] = tf.add(self.tmp[i], self.g2[i])\n",
    "            self.tmp[i] = tf.add(self.tmp[i], 0.01)\n",
    "            self.tmp[i] = tf.sqrt(self.tmp[i])\n",
    "            self.deltas[i] = tf.multiply(self.deltas[i], 0) + tf.multiply(tf.divide(dw[i], self.tmp[i]), self.lr)\n",
    "            tmp_weights.append(tf.add(self.network.weights[i], self.deltas[i]))\n",
    "\n",
    "        self.network.set_weights(tmp_weights)\n",
    "\n",
    "    def learn_with_experience_replay(self):\n",
    "        try:\n",
    "            return_val = 0  # for TensorBoard\n",
    "\n",
    "            preprocessed_input = self.env.initialize_input_sequence()\n",
    "\n",
    "            while self.num_steps < self.total_steps:\n",
    "                self.num_steps += 1\n",
    "\n",
    "                if self.env.is_game_over():\n",
    "                    preprocessed_input = self.env.initialize_input_sequence()\n",
    "\n",
    "                    # Write total return of the last episode to TensorBoard\n",
    "                    with self.scalar_file_writer.as_default():\n",
    "                        tf.summary.scalar('Return', return_val, step=self.num_steps)\n",
    "                        tf.summary.flush()\n",
    "\n",
    "                    return_val = 0  # Reset return value\n",
    "\n",
    "                action = self.e_greedy_select_action(preprocessed_input)\n",
    "                reward, next_preprocessed_input = self.env.execute_action(action)\n",
    "\n",
    "                return_val += reward  # Accumulate returns of the running episode in return_val\n",
    "\n",
    "                self.experience_replay_memory.add(preprocessed_input, action, reward, next_preprocessed_input,\n",
    "                                                  self.env.is_game_over())\n",
    "\n",
    "                preprocessed_input = next_preprocessed_input\n",
    "\n",
    "                # perform gradient descent step\n",
    "                if (self.num_steps > self.learn_start) and (self.num_steps % self.update_freq == 0):\n",
    "                    s, a, r, s2, term = self.experience_replay_memory.sample(self.minibatch_size)\n",
    "                    self.q_learn_minibatch(s, a, r, s2, term)\n",
    "\n",
    "                    # Write weight histograms to Tensorboard\n",
    "                    if self.write_weight_histogram:\n",
    "                        with self.histogram_file_writer.as_default():\n",
    "                            tf.summary.histogram('Layer_0 (Conv) Weights', self.network.layers[0].weights[0],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_0 (Conv) Bias', self.network.layers[0].weights[1],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_1 (Conv) Weights', self.network.layers[1].weights[0],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_1 (Conv) Bias', self.network.layers[1].weights[1],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_2 (Conv) Weights', self.network.layers[2].weights[0],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_2 (Conv) Bias', self.network.layers[2].weights[1],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_4 (Dense) Weights', self.network.layers[4].weights[0],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_4 (Dense) Bias', self.network.layers[4].weights[1],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_5 (Dense) Weights', self.network.layers[5].weights[0],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.histogram('Layer_5 (Dense) Bias', self.network.layers[5].weights[1],\n",
    "                                                 step=self.num_steps)\n",
    "                            tf.summary.flush()\n",
    "\n",
    "                # update target-q network\n",
    "                if self.target_q is not None and self.num_steps % self.target_q == 1:\n",
    "                    self.target_network = tf.keras.models.clone_model(self.network)\n",
    "\n",
    "                with self.scalar_file_writer.as_default():\n",
    "                    tf.summary.scalar('Reward', reward, step=self.num_steps)\n",
    "                    tf.summary.scalar('epsilon', self.epsilon, step=self.num_steps)\n",
    "                    tf.summary.flush()\n",
    "\n",
    "                if (self.num_steps % self.save_model_steps) == 0:\n",
    "                    self.network.save(filepath=self.save_model_path + str(self.num_steps))\n",
    "\n",
    "        except Exception as exception:\n",
    "            print(exception)\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            # Close file writers\n",
    "            self.scalar_file_writer.close()\n",
    "            self.histogram_file_writer.close()\n",
    "\n",
    "            # Save model\n",
    "            self.network.save(filepath=self.save_model_path + str(self.num_steps) + \"saved_from_finally_block\")\n",
    "\n",
    "            agent_state = {\n",
    "                \"num_steps\": self.num_steps,\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"lr\": self.lr\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(self.save_model_path,'agent_state.json'), 'w') as f:\n",
    "                json.dump(agent_state, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgentArgs(object):\n",
    "    def __init__(self):\n",
    "        self.env = None\n",
    "        self.network = None\n",
    "        self.total_steps = 50000000\n",
    "        self.update_freq = 4\n",
    "        self.learn_start = 50000\n",
    "        self.epsilon_start = 1.  # Affects epsilon update\n",
    "        self.epsilon_end = 0.1\n",
    "        self.epsilon_endt = 1000000\n",
    "        self.discount = 0.99\n",
    "        self.rescale_r = None\n",
    "        self.r_max = None\n",
    "        self.minibatch_size = 32\n",
    "        self.target_q = 10000\n",
    "\n",
    "        # learning reate annealing\n",
    "        self.lr = None\n",
    "        self.lr_end = None\n",
    "        self.lr_endt = None\n",
    "\n",
    "        self.save_model_steps = 10000\n",
    "        self.save_model_path = None\n",
    "\n",
    "        self.write_weight_histogram = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = tf.keras.models.load_model('models/612872saved_from_finally_block/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save(\"models/612872saved_from_finally_block-h5\", save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(network.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ALEManager(ALEManagerArgs())\n",
    "agent_args = DQLAgentArgs()\n",
    "agent_args.env = env\n",
    "\n",
    "dql_agent = DeepQLearningAgent(args=agent_args)\n",
    "dql_agent.learn_with_experience_replay()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
